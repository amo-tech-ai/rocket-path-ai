# Idea Discovery

> **Pipeline stage:** ExtractorAgent | **Merged from:** startup-ideation, lean-canvas, creative-intelligence

Structured frameworks for generating startup ideas, framing problems, modeling businesses on one page, and evaluating opportunities. Use when someone is brainstorming business concepts, exploring a problem space, building a Lean Canvas, conducting early-stage research, or deciding whether an idea is worth pursuing.

---

## 1. Idea Generation Frameworks

### The "Why Now" Test

Every viable startup idea needs a clear answer to: **"What has changed that makes this idea newly possible?"**

Categories of change that create opportunities:
- **Technology shifts** -- new capabilities (AI, Web3, new APIs, cheaper compute)
- **Behavior shifts** -- changed habits (remote work, subscription preference, mobile-first)
- **Infrastructure changes** -- new platforms, new distribution rails
- **Regulatory changes** -- new rules that open or close markets

If no "Why Now" exists, the idea likely has bad timing -- either too early or too late.

### Personal Experience Method

The best startup ideas come from unique personal experience and access that others lack. Key questions:

- What problem have you personally experienced that frustrated you deeply?
- What do you know or have access to that most people don't?
- What domain expertise gives you an unfair insight?

**Anti-pattern: Starting from trends instead of problems.** Chasing hot topics (AI, crypto) without a specific problem leads to undifferentiated products.

### Information Diet Differentiation

If every founder reads the same articles and follows the same people, every founder has the same ideas. Unique insights require unique inputs:

- Consume information from sources most founders ignore
- Talk to users in niches you have personal access to
- Study adjacent industries for transplantable patterns

### Tarpit Identification

Certain idea categories attract many founders repeatedly despite low success rates. Warning signs:

- Hundreds of companies have tried and failed in the same space
- The idea sounds obviously good to everyone (high competition signal)
- No clear "Why Now" differentiator
- The problem is real but the market dynamics prevent sustainable businesses

**Test:** "How many other startups are working on something similar, and why did they fail?"

---

## 2. Problem Framing Techniques

### 5 Whys -- Root Cause Analysis

Dig beneath surface symptoms to find underlying causes. Start with a problem statement, ask "Why?" iteratively until you reach the root cause (usually 3-5 iterations).

```
Problem: Website conversion rate is low

Why? -> Users abandon at checkout
Why? -> Checkout form is too long
Why? -> We collect unnecessary information
Why? -> No one reviewed required vs. optional fields
Why? -> No process for form design review

Root Cause: Lack of UX review process
```

**Tips:**
- The "five" is a guideline -- stop when you reach the root cause
- Can branch into multiple "why" paths for multiple causes
- Verify each "why" with data when possible
- **Variation -- 5 Hows:** Start with a goal and ask "How?" to find implementation steps

### SCAMPER -- Creative Variation

Generate creative variations by systematically modifying existing concepts. Apply each prompt to your base concept (product, feature, process):

| Letter | Prompt | Question |
|--------|--------|----------|
| **S** | Substitute | What can you replace or swap out? (materials, tech, people) |
| **C** | Combine | What can you merge? (features, products, markets) |
| **A** | Adapt | What can you borrow from other industries or domains? |
| **M** | Modify/Magnify/Minify | What can you change in size, scope, frequency? |
| **P** | Put to other uses | What new audiences or applications? |
| **E** | Eliminate | What can you remove to simplify? |
| **R** | Reverse/Rearrange | What if you flipped the sequence or swapped roles? |

**Application:** Generate 3-5 ideas per letter, then select most promising across all 7. Combine modifications for novel solutions ("What if we SUBSTITUTE X and REVERSE Y?").

### Six Thinking Hats -- Multi-Perspective Analysis

Edward de Bono's framework. Analyze from six distinct perspectives, 5-7 minutes each:

| Hat | Focus | Key Questions |
|-----|-------|---------------|
| **White** | Facts and data | What information do we have? What's missing? |
| **Red** | Emotions and intuition | How do I feel about this? What's my gut saying? |
| **Black** | Risks and caution | What could go wrong? What are the weaknesses? |
| **Yellow** | Benefits and optimism | What are the advantages? Best-case scenario? |
| **Green** | Creativity and alternatives | What else could we try? What if...? |
| **Blue** | Process and synthesis | What have we learned? What's the decision? |

**Decision-making sequence:** Blue -> White -> Green -> Yellow -> Black -> Red -> Blue

**Evaluation sequence:** Blue -> White -> Yellow -> Black -> Red -> Green -> Blue

### Reverse Brainstorming -- Failure Mode Discovery

Identify risks by asking "How could we make this fail completely?"

1. State the goal
2. Reverse it: brainstorm all ways to cause failure
3. For each failure mode, identify the preventive action
4. Prioritize mitigations by impact and likelihood

**Variations:**
- **Pre-mortem:** Imagine the project failed, then ask why
- **Red Team:** One group tries to break what another group built

### Starbursting -- Question Exploration

Explore a topic comprehensively by generating questions from six prompts before seeking answers:

| Prompt | Example Questions |
|--------|-------------------|
| **Who** | Who benefits? Who will use this? Who has done this before? Who might oppose? |
| **What** | What problem does this solve? What resources are needed? What could go wrong? |
| **Where** | Where will this be used? Where are competitors? Where are the gaps? |
| **When** | When is the market ready? When do users need this? When should we launch? |
| **Why** | Why now? Why will users care? Why us vs. competitors? Why might this fail? |
| **How** | How will users discover it? How will we measure success? How will we scale? |

Generate 5-10 questions per prompt before answering any. Prioritize which questions need answers before proceeding.

### Technique Selection Quick Reference

| Need | Primary Technique | Alternative |
|------|-------------------|-------------|
| Find root cause | 5 Whys | Starbursting |
| Generate feature ideas | SCAMPER | Mind Mapping |
| Identify risks | Reverse Brainstorming | SWOT (Threats) |
| Make important decision | Six Thinking Hats | SWOT |
| Explore all angles | Starbursting | Six Thinking Hats |
| Strategic positioning | SWOT Analysis | Six Thinking Hats |

**Powerful combinations:**
1. **SCAMPER -> Mind Mapping** -- generate ideas, then organize hierarchically
2. **Starbursting -> 5 Whys** -- generate questions, then deep-dive on key ones
3. **Reverse Brainstorming -> Six Thinking Hats** -- identify risks, then evaluate mitigations
4. **SWOT -> SCAMPER** -- identify opportunities, then generate innovative approaches

---

## 3. Lean Canvas

### Foundation

| Aspect | Details |
|--------|---------|
| **Source** | Ash Maurya -- "Running Lean" (2012), adapted from Osterwalder's Business Model Canvas |
| **Core Principle** | Document your Plan A, identify the riskiest parts, and systematically test them |
| **Key Difference from BMC** | Replaces Partners/Resources/Activities with Problem/Solution/Key Metrics; adds Unfair Advantage; focuses on risk and learning |

### The 9 Boxes

```
+-----------------------+-----------------------+-----------------------+
|                       |                       |                       |
|  2. PROBLEM           |  4. SOLUTION          | 3. UNIQUE VALUE       |
|  (Top 3)              |  (Top 3 features)     |    PROPOSITION        |
|                       |                       |                       |
|                       +-----------------------+ High-level concept    |
|                       |                       |                       |
| Existing              |  8. KEY METRICS       |                       |
| Alternatives          |  (AARRR)              |                       |
|                       |                       |                       |
+-----------------------+-----------------------+-----------------------+
|                       |                       |                       |
|  9. UNFAIR            |  5. CHANNELS          | 1. CUSTOMER           |
|     ADVANTAGE         |  (Path to customers)  |    SEGMENTS           |
|  (Can't be copied)    |                       |  (Target users)       |
|                       |                       |                       |
|                       |                       | Early Adopters        |
+-----------------------+-----------------------+-----------------------+
|  7. COST STRUCTURE                |  6. REVENUE STREAMS               |
|  (Fixed + Variable)              |  (Pricing model)                  |
+-----------------------------------+-----------------------------------+
```

**Recommended fill order:** Customer Segments -> Problem -> UVP -> Solution -> Channels -> Revenue -> Cost -> Key Metrics -> Unfair Advantage

### Box-by-Box Guide

**1. Customer Segments**
- Be specific: not "businesses" but "SaaS companies 10-50 employees"
- Identify early adopters who feel the pain most acutely
- If you can't describe them, you can't find them

**2. Problem (Top 3)**
- List from the CUSTOMER's perspective, not yours
- Document existing alternatives (how they solve it today)
- If existing alternatives work well, your problem isn't painful enough

**3. Unique Value Proposition**
- Formula: "We help [segment] [achieve outcome] unlike [alternatives] because [differentiator]"
- High-level concept analogy: "X for Y" (e.g., "Uber for dog walkers")
- Focus on END BENEFIT, not features. Test: can you say it in 10 seconds?

**4. Solution (Top 3 features)**
- Match each feature to a specific problem
- Keep it minimal -- MVP thinking
- This box should be the LAST one you fill with certainty

**5. Channels**
- Path: Awareness -> Acquisition -> Retention
- Start with channels that don't scale (do things that don't scale)
- Match channels to where early adopters spend time
- Free channels first; paid channels after product-market fit

**6. Revenue Streams**
- Models: subscription, one-time, freemium, transaction fee, advertising
- Revenue formula: [Customers] x [Price] x [Frequency] = [Revenue]
- Price on value, not cost. Test pricing early.

**7. Cost Structure**
- Fixed costs (monthly): team, infrastructure, tools
- Variable costs (per customer): hosting, API calls, support
- CAC target and break-even calculation
- Rule: CAC must be < LTV

**8. Key Metrics (AARRR)**
- **A**cquisition: How many sign up?
- **A**ctivation: How many have "aha" moment?
- **R**etention: How many come back?
- **R**evenue: How many pay?
- **R**eferral: How many refer others?
- Identify One Metric That Matters (OMTM) for current stage
- Vanity metrics (signups, page views) lie -- measure behavior

**9. Unfair Advantage**
- Must be something that can't be easily copied or bought
- Valid: insider information, dream team, network effects, community, proprietary data, SEO ranking
- NOT valid: "passion," "first mover advantage"
- Most startups don't have one at first -- it often emerges over time

### Solution-First Bias Detection

Watch for these signs that the canvas was built backward (solution-first instead of problem-first):

- Problem box restates the solution as a negative ("Users don't have our product")
- Customer segment is too broad ("all businesses," "everyone")
- UVP describes features instead of outcomes
- Existing alternatives are missing or dismissed without evidence
- Solution box was filled first and with the most detail

**Fix:** Strip the solution, rewrite the problem from the customer's perspective, validate that the problem exists independently of your solution.

### Canvas Scoring Algorithm

Rate each box 1-5 on specificity and evidence:

| Score | Meaning |
|-------|---------|
| 1 | Empty or generic (e.g., "large market") |
| 2 | Specified but unvalidated assumption |
| 3 | Specific hypothesis with reasoning |
| 4 | Hypothesis with indirect evidence (research, analogies) |
| 5 | Validated with direct evidence (interviews, data, experiments) |

**Canvas health:** Sum all 9 boxes. Score < 18 = too vague to act on. Score 18-27 = testable hypotheses. Score 28-36 = partially validated. Score 37-45 = strong foundation.

### Assumption Extraction and Risk Assessment

**Stage 1 -- Product Risk: "Do I have a problem worth solving?"**

| Assumption | Type | Risk Level |
|------------|------|------------|
| Problem exists and is painful | PROBLEM | High/Med/Low |
| Customers are identifiable and reachable | CUSTOMER | High/Med/Low |
| Current alternatives are inadequate | PROBLEM | High/Med/Low |

**Stage 2 -- Market Risk: "Have I built something people want?"**

| Assumption | Type | Risk Level |
|------------|------|------------|
| Solution solves the problem | SOLUTION | High/Med/Low |
| Customers will pay [price] | REVENUE | High/Med/Low |
| CAC is sustainable | COST | High/Med/Low |

**Stage 3 -- Scale Risk: "Can I build a viable business?"**

| Assumption | Type | Risk Level |
|------------|------|------------|
| Channels work at scale | CHANNEL | High/Med/Low |
| Unit economics work (LTV > 3x CAC) | COST/REV | High/Med/Low |
| Defensibility exists or can be built | ADVANTAGE | High/Med/Low |

For each risky assumption, design a validation experiment:

```
Assumption: [Statement]
Risk if wrong: [Consequence]
Experiment type: Interview / Landing page / Prototype / Concierge
Target: [Who / How many]
Timeline: [Duration]
Validated if: [Specific metric]
Invalidated if: [Specific metric]
```

### Canvas Versioning

Track evolution as you learn:

```
Version 1.0 - [Date]: Initial hypothesis. Key assumptions: [list]
Version 1.1 - [Date]: Updated [boxes]. Evidence: [what you learned]
Version 2.0 - [Date]: Major pivot. [Customer/problem/solution] changed because [evidence]
```

---

## 4. Research Methods

### Market Research

**Key questions:** Market size (TAM/SAM/SOM), growth trends, customer segments, barriers to entry.

**Market sizing formulas:**

```
TAM = [Total potential customers] x [Average revenue per customer]
SAM = TAM x [% you can realistically serve given constraints]
SOM = SAM x [Realistic market share % in timeframe]
```

**Growth analysis data points:**
- Historical CAGR (3-5 years)
- Projected growth rate (analyst forecasts)
- Growth drivers and inhibitors

**Segmentation approaches:**
- Demographic (age, income, education)
- Geographic (location, urban/rural)
- Psychographic (values, attitudes)
- Behavioral (usage, purchase patterns)
- Firmographic (company size, industry, revenue -- B2B)

**Sources:** Gartner, Forrester, IDC, Statista, IBISWorld, Census Bureau, BLS, SEC filings, trade associations.

### Competitive Research

**Competitor categories:**
- **Direct:** Same solution, same market
- **Indirect:** Different solution, same problem
- **Potential:** Has capability/resources to enter

**Information to collect per competitor:**
- Company: founding date, team size, funding (Crunchbase), investors
- Product: core features, platform, integrations, API
- Pricing: tiers, per-seat cost, free trial, annual vs. monthly
- Position: target customers, customer count, key accounts, geography
- Feedback: review ratings (G2, Capterra), common praises, common complaints

**Feature comparison matrix:**

| Feature | Us | Competitor A | Competitor B | Gap? |
|---------|-----|-------------|-------------|------|
| Feature 1 | Y | Y | N | -- |
| Feature 2 | Y | N | N | Opportunity |
| Feature 3 | N | Y | Y | We're behind |

**Positioning matrix axes:** Price (low-high) vs. Capability (basic-advanced). Plot each competitor to find white space.

**Sources:** G2, Capterra, TrustRadius, Product Hunt, Crunchbase, competitor websites, LinkedIn, Reddit, Hacker News, industry publications.

### Technical Research

**Evaluation criteria for technology choices:**

| Criterion | Weight | What to Assess |
|-----------|--------|----------------|
| Maturity | High | Version history, breaking changes, backward compat |
| Community | High | GitHub stars, contributors, SO questions, corporate backing |
| Performance | High | Benchmarks, scalability, resource requirements |
| Documentation | Medium | Official docs quality, tutorials, examples |
| Learning Curve | Medium | Setup complexity, dev workflow, IDE support |
| Ecosystem | High | Plugins, integrations, package ecosystem |

**Sources:** Official docs, GitHub, Stack Overflow, ThoughtWorks Technology Radar, State of JS/CSS surveys, Dev.to, HN.

### User Research

**Method selection:**

| Question | Method | Timeline | Sample Size |
|----------|--------|----------|-------------|
| Do users need this? | Survey + Interviews | 1-2 weeks | 50 survey, 8 interviews |
| Is the UI intuitive? | Usability Testing | 1 week | 5-8 users (finds 85% of issues) |
| Which design preferred? | A/B Test | 2-4 weeks | 1000+ per variant |
| Common pain points? | Review Mining | 2-3 days | 100 reviews |

**Interview structure (50 min total):**
- Introduction + rapport (5 min)
- Warm-up / background (5 min)
- Main questions -- open-ended, probe for details (35 min)
- Wrap-up (5 min)

**Critical rules:** Avoid leading questions. "Don't you think this feature would be useful?" is bad. "How do you currently handle [problem]?" is good.

### Research Time-Boxing

| Depth | Duration | Sources | Output |
|-------|----------|---------|--------|
| Quick | 1-2 hours | 3-5 sources, secondary only | Summary findings |
| Standard | 3-6 hours | 10-15 sources, secondary + analysis | Structured report |
| Comprehensive | 1-2 days | 20+ sources, secondary + primary | Full report with matrices |

---

## 5. Idea Evaluation Checklist

### Completeness Check

- [ ] Customer segment is specific (not generic)
- [ ] Early adopters identified with reason they'll buy first
- [ ] Top 3 problems listed from customer perspective
- [ ] Existing alternatives researched (not guessed)
- [ ] "Why Now" has a clear answer
- [ ] UVP is one sentence, focused on end benefit
- [ ] Solution maps features to specific problems
- [ ] Revenue model makes mathematical sense (LTV > 3x CAC)
- [ ] Key metric identified for current stage
- [ ] Unfair advantage is honest ("none yet" is acceptable)

### Red Flags

- [ ] Starting from trends instead of problems
- [ ] Solution-first bias (problem restates the solution as a negative)
- [ ] Customer segment = "everyone" or "all businesses"
- [ ] No clear "Why Now" differentiator
- [ ] Tarpit pattern (many attempts, few successes in space)
- [ ] Identical information diet as every other founder
- [ ] Revenue assumptions untested
- [ ] "Passion" or "first mover" listed as unfair advantage

### Kill-or-Continue Questions

1. What would have to be true for this to be a billion-dollar business?
2. How many other startups are working on something similar, and why did they fail?
3. Why is this idea possible now when it wasn't possible two years ago?
4. Can you describe the early adopter in one sentence?
5. Would the early adopter pay today, before the product is built?

### SWOT Template for Final Evaluation

```
SWOT Analysis: [Idea Name]

INTERNAL                          EXTERNAL
+------------------+              +------------------+
| STRENGTHS        |              | OPPORTUNITIES    |
| 1.               |              | 1.               |
| 2.               |              | 2.               |
| 3.               |              | 3.               |
+------------------+              +------------------+
| WEAKNESSES       |              | THREATS          |
| 1.               |              | 1.               |
| 2.               |              | 2.               |
| 3.               |              | 3.               |
+------------------+              +------------------+

STRATEGIES:
SO (Strength + Opportunity): [Use strengths to capture opportunities]
ST (Strength + Threat):      [Use strengths to mitigate threats]
WO (Weakness + Opportunity): [Overcome weaknesses to pursue opportunities]
WT (Weakness + Threat):      [Minimize weaknesses and avoid threats]
```

---

## 6. Diagnostic Workflow

When helping a founder with ideation, follow this structured approach:

1. **Understand their background** -- Ask about personal experience, skills, domain expertise, and what problems they have encountered firsthand. The best ideas come from unique access and insight.
2. **Explore information sources** -- Discuss what they read, who they talk to, and whether their information diet is differentiated from the typical founder's.
3. **Apply the Why Now test** -- Help them identify what has changed (technology, behavior, infrastructure, regulation) that makes this idea newly possible or newly viable.
4. **Identify tarpit risks** -- Flag common idea patterns that attract many founders but rarely succeed. Ask: "How many others have tried this, and why did they fail?"
5. **Frame with Lean Canvas** -- If the idea survives steps 1-4, build a quick canvas to expose hidden assumptions and identify the riskiest parts of the plan.

### Quick-Reference Diagnostic Questions

Use these to rapidly assess an idea in a conversation:

| Stage | Question | What It Reveals |
|-------|----------|-----------------|
| Origin | "Where did this idea come from -- a problem you lived, or a trend you read about?" | Solution-first vs. problem-first bias |
| Uniqueness | "What do you know or have access to that most founders don't?" | Unfair insight / founder-market fit |
| Timing | "Why is this possible now when it wasn't two years ago?" | Why Now strength |
| Competition | "How many startups have tried this, and why did they fail?" | Tarpit risk |
| Scale | "What would have to be true for this to be a billion-dollar business?" | Ambition calibration |
| Urgency | "Would the early adopter pay today, before the product exists?" | Problem severity |
| Specificity | "Can you describe your early adopter in one sentence?" | Customer clarity |

### Common Founder Mistakes

Flag these patterns early -- they are the most frequent causes of wasted effort:

1. **Starting from trends instead of problems** -- Chasing hot topics (AI, crypto) without a specific painful problem leads to undifferentiated products that nobody urgently needs.
2. **Identical information diet** -- Reading the same TechCrunch articles and following the same Twitter accounts as every other founder produces the same ideas. Unique inputs produce unique insights.
3. **Ignoring the Why Now** -- Ideas without a clear reason they are newly possible often indicate missed timing -- either too early or too late.
4. **Tarpit ideas** -- Certain categories (social networks, to-do apps, local marketplaces) attract hundreds of founders despite consistent failure. High obviousness + low success rate = tarpit.
5. **Solution-first thinking** -- Building a product and then searching for a problem to attach to it. The problem box on the canvas should never be a negation of the solution.
6. **"Everyone" as customer** -- If your customer segment includes "all businesses" or "everyone," you have not done the work to identify who feels the pain most acutely.
7. **Confusing passion for advantage** -- "I'm passionate about this" is not an unfair advantage. Neither is "first mover advantage" (which rarely exists and even more rarely persists).

### YC Philosophy References

Key principles from Y Combinator partners relevant to ideation:

- **Dalton Caldwell:** "Try to go more off the beaten path either from your personal experience." Unique personal experience and access are the strongest foundations for startup ideas. Avoid starting from popular trends that everyone is chasing.
- **Paul Graham:** "Live in the future, then build what's missing." The best founders notice problems that will affect many people but that few people see yet.
- **Michael Seibel:** "Talk to users before you write a line of code." Validation starts with conversations, not prototypes.
- **Jessica Livingston:** "Startups are about making something people want." If you skip the problem-discovery phase, no amount of execution will save you.

---

## 7. Cross-Phase Applicability

The techniques in this skill apply beyond the initial ideation phase. Use this matrix to identify which frameworks are relevant at each stage of a startup's lifecycle:

### Phase 1: Discovery and Analysis

- Market research for product discovery (Section 4)
- Competitive landscape analysis (Section 4)
- Problem exploration using 5 Whys (Section 2)
- User research and needs analysis (Section 4)
- Lean Canvas for business model hypothesis (Section 3)

### Phase 2: Planning and Strategy

- Feature brainstorming with SCAMPER (Section 2)
- SWOT analysis for strategic positioning (Section 5)
- Risk identification with Reverse Brainstorming (Section 2)
- Assumption extraction and experiment design (Section 3)
- Prioritization through canvas scoring (Section 3)

### Phase 3: Solutioning and Design

- Architecture alternatives exploration with Six Thinking Hats (Section 2)
- Design pattern research (Section 4 -- Technical Research)
- Mind Mapping for system organization (Section 2)
- Technical research for implementation approaches (Section 4)

### Phase 4: Implementation and Iteration

- Technical solution research (Section 4)
- Best practices investigation (Section 4)
- Problem-solving with structured brainstorming techniques (Section 2)
- Canvas versioning to track pivots and learnings (Section 3)

---

## 8. Parallel Agent Workflows

For complex ideation sessions, leverage parallel subagents to maximize throughput. Each agent applies one technique independently, and results are synthesized afterward.

### Multi-Technique Brainstorming (Fan-Out)

**Pattern:** Launch 3-6 parallel agents, each applying one brainstorming technique to the same objective.

| Agent | Technique | Output |
|-------|-----------|--------|
| Agent 1 | SCAMPER -- generate creative feature variations | brainstorm-scamper.md |
| Agent 2 | Mind Mapping -- organize ideas hierarchically | brainstorm-mindmap.md |
| Agent 3 | Reverse Brainstorming -- identify failure modes and risks | brainstorm-risks.md |
| Agent 4 | Six Thinking Hats -- multi-perspective analysis | brainstorm-hats.md |
| Agent 5 | Starbursting -- explore via question generation | brainstorm-questions.md |
| Agent 6 | SWOT Analysis -- strategic positioning | brainstorm-swot.md |

**Coordination steps:**
1. Define the brainstorming objective and context clearly.
2. Select 3-6 complementary techniques based on the objective.
3. Launch parallel agents, each applying one technique.
4. Each agent generates 10-30 ideas/insights using their technique.
5. Main context synthesizes all outputs into a unified report.
6. Extract top 3-5 actionable insights across all techniques.

### Comprehensive Research (Fan-Out)

**Pattern:** Launch 4 parallel agents, each covering one research domain.

| Agent | Domain | Output |
|-------|--------|--------|
| Agent 1 | Market research -- size, trends, opportunities | research-market.md |
| Agent 2 | Competitive analysis -- competitors, features, gaps | research-competitive.md |
| Agent 3 | Technical research -- technologies, patterns, approaches | research-technical.md |
| Agent 4 | User research -- needs, pain points, workflows | research-user.md |

**Coordination steps:**
1. Define research scope and questions.
2. Launch all 4 research agents in parallel.
3. Each agent uses WebSearch/WebFetch for their domain.
4. Agents document findings with sources and quantitative data.
5. Main context synthesizes into comprehensive research report.
6. Generate actionable recommendations from combined insights.

### Problem Exploration (Fan-Out)

**Pattern:** Launch 3 parallel agents to explore a problem from different analytical angles.

| Agent | Approach | Output |
|-------|----------|--------|
| Agent 1 | 5 Whys -- uncover root causes | exploration-5whys.md |
| Agent 2 | Starbursting -- generate comprehensive questions | exploration-questions.md |
| Agent 3 | Stakeholder perspective analysis | exploration-perspectives.md |

**Coordination steps:**
1. Write a clear problem statement.
2. Launch parallel agents for deep problem exploration.
3. Each agent explores the problem from a different analytical angle.
4. Main context identifies true root causes and key questions.
5. Generate a prioritized problem definition with insights.

### Solution Generation (Fan-Out)

**Pattern:** Launch 4 parallel agents to generate and evaluate solution alternatives.

| Agent | Task | Output |
|-------|------|--------|
| Agent 1 | Generate solution variations using SCAMPER | solutions-scamper.md |
| Agent 2 | Research existing solutions and best practices | solutions-research.md |
| Agent 3 | Identify constraints and feasibility considerations | solutions-constraints.md |
| Agent 4 | Create evaluation criteria for solution selection | solutions-criteria.md |

**Coordination steps:**
1. Load the problem definition from the exploration phase.
2. Launch parallel agents for solution exploration.
3. Collect diverse solution approaches and variations.
4. Main context evaluates solutions against criteria.
5. Generate prioritized solution recommendations.

---

## 9. Output Templates

### Brainstorming Session Report

```
# Brainstorming Session: [Topic]

**Date:** [Date]
**Objective:** [What we are trying to discover or solve]
**Techniques Used:** [List with rationale for selection]

## Ideas Generated

### Category 1: [Name]
- Idea 1.1: [Description] -- [Potential value]
- Idea 1.2: [Description] -- [Potential value]

### Category 2: [Name]
- Idea 2.1: [Description] -- [Potential value]

## Top Insights (3-5)

1. [Insight] -- [Why it matters] -- [Suggested action]
2. [Insight] -- [Why it matters] -- [Suggested action]
3. [Insight] -- [Why it matters] -- [Suggested action]

## Risks Identified

- [Risk] -- [Mitigation]

## Recommended Next Steps

1. [Action] -- [Owner] -- [Timeline]
2. [Action] -- [Owner] -- [Timeline]
```

### Research Report

```
# Research Report: [Topic]

**Date:** [Date]
**Scope:** [What questions this research answers]
**Methodology:** [Methods and sources used]

## Key Findings

### Theme 1: [Name]
- Finding: [Statement with data]
- Source: [Citation]
- Implication: [What this means for us]

### Theme 2: [Name]
- Finding: [Statement with data]
- Source: [Citation]
- Implication: [What this means for us]

## Competitive Matrix (if applicable)

| Feature | Us | Competitor A | Competitor B |
|---------|-----|-------------|-------------|
| Feature 1 | Y | Y | N |

## Recommendations

1. [Recommendation] -- [Rationale] -- [Priority]
2. [Recommendation] -- [Rationale] -- [Priority]

## Open Questions

- [Question that needs further investigation]
```

---

## 10. Best Practices Checklist

Follow these principles when running ideation and research sessions:

1. **Use structured frameworks, not free-form brainstorming.** Random ideation produces scattered results. Proven techniques produce actionable insights.
2. **Diverge before converging.** Generate quantity first, filter for quality second. Never critique during the generation phase.
3. **Apply multiple techniques for coverage.** No single technique catches everything. Combine 2-3 complementary approaches (see Technique Selection in Section 2).
4. **Document all ideas, even weak ones.** Filtering comes later. A "weak" idea can spark a strong one in synthesis.
5. **Quantify findings when possible.** Market size in dollars, customer count, percentage growth, feature counts. Numbers make insights concrete and comparable.
6. **Time-box every session.** Open-ended brainstorming loses focus. Set a time limit per technique (15-45 minutes depending on complexity).
7. **Provide actionable insights, not raw data dumps.** Every research finding should answer: "So what? What should we do about this?"
8. **Reference sources for all research findings.** Unattributed claims are just opinions. Cite where data comes from.
9. **Extract clear next steps at the end of every session.** The output of ideation is not a list of ideas -- it is a prioritized set of actions.
10. **Categorize results for easier decision-making.** Group by theme, innovation level (incremental vs. breakthrough), or risk level.

---

## References

- Maurya, Ash. "Running Lean" (2012) -- Lean Canvas methodology
- Maurya, Ash. "Scaling Lean" (2016) -- Traction roadmap
- Osterwalder, Alex. "Business Model Generation" (2010) -- Original BMC
- Blank, Steve. "The Startup Owner's Manual" (2012) -- Customer Development
- Ries, Eric. "The Lean Startup" (2011) -- Build-Measure-Learn
- de Bono, Edward. "Six Thinking Hats" (1985) -- Parallel thinking
- Caldwell, Dalton -- YC: off-the-beaten-path ideas, tarpit avoidance, information diet
- Hoover, Ryan -- "Why Now" analysis, technology/behavior shift frameworks
- Graham, Paul -- "Live in the future, then build what's missing"
- Seibel, Michael -- "Talk to users before you write a line of code"

## Related Skills

- `startup/market-validation` -- Measuring product-market fit
- `startup/lean-experiments` -- Experiment design and execution
- `startup/pitch-deck` -- Communicating the idea to investors
- `startup/traction-metrics` -- Tracking growth after launch
